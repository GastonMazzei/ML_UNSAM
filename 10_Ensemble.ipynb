{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons, make_hastie_10_2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciones útiles\n",
    "\n",
    "def print_results(clfrgr):\n",
    "    print('#####')\n",
    "    print('{}.'.format(type(clfrgr).__name__))\n",
    "                       \n",
    "    train_score = clfrgr.score(X_train, y_train)\n",
    "    test_score = clfrgr.score(X_test, y_test)\n",
    "    print('Train score: ', train_score)\n",
    "    print('Test score: ', test_score)\n",
    "\n",
    "def print_results_boost(clfrgr):\n",
    "    print('#####')\n",
    "    print('{} con {} estimadores de tipo {}, con max_depth={}.'.format(type(clfrgr).__name__,\n",
    "                                                                       clfrgr.n_estimators, \n",
    "                                                                       type(clfrgr.base_estimator).__name__,\n",
    "                                                                       clfrgr.base_estimator.max_depth))\n",
    "    train_score = clfrgr.score(X_train, y_train)\n",
    "    test_score = clfrgr.score(X_test, y_test)\n",
    "    print('Train score: ', train_score)\n",
    "    print('Test score: ', test_score)\n",
    "\n",
    "    return train_score, test_score\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"o\", color=cmap(0), alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"yo\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(2000, shuffle=True, noise=0.2, random_state=1234)\n",
    "size_train = int(0.8 * len(X))\n",
    "X_train, X_test = X[:size_train], X[size_train:]\n",
    "y_train, y_test = y[:size_train], y[size_train:]\n",
    "\n",
    "plt.scatter(*X_train.T, c=y_train, marker='.')\n",
    "#plt.scatter(*X_test.T, c=y_test, marker='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=12, random_state=107)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "# svm_pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "#                            ('svc', SVC(kernel='poly'))])\n",
    "# svm_pipe = svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(dt, X_train, y_train, contour=True, alpha=0.1)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(max_depth=12, random_state=107), n_estimators=500, n_jobs=-1, \n",
    "                        oob_score=True, max_samples=1000, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.oob_decision_function_.shape)\n",
    "print(clf.oob_score_)\n",
    "print(clf.oob_decision_function_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf, X_train, y_train, alpha=0.1)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "dt = ExtraTreeClassifier(max_depth=12, random_state=107)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "# svm_pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "#                            ('svc', SVC(kernel='poly'))])\n",
    "# svm_pipe = svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(dt, X_train, y_train, contour=True, alpha=0.1)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(ExtraTreeClassifier(max_depth=12, random_state=107), n_estimators=500, n_jobs=-1, \n",
    "                        oob_score=False, max_samples=1000, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(clf, X_train, y_train, alpha=0.1)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "(mostrado con *bagging*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el efecto de usar modelos con más o menos flexibilidad a la hora de realizar comités. \n",
    "\n",
    "En este caso, arranquemos con un DT, con profundidad 1. Obviamente, no va a alcanzar para explicar este dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=1, random_state=107)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "# dt = ExtraTreeClassifier(max_depth=7, random_state=107)\n",
    "\n",
    "# svm_pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "#                            ('svc', SVC(kernel='rbf', gamma=1.0, C=0.6))])\n",
    "# svm_pipe = svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(dt, X_train, y_train, contour=True, alpha=0.1)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, ¿qué pasará si combinamos 500 de estos? ¿Lo lograremos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(dt, n_estimators=2000, n_jobs=-1, \n",
    "                        oob_score=False, max_samples=0.2, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(clf, X_train, y_train, alpha=0.1)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No no no!\n",
    "\n",
    "Volvamos arriba y veamos qué nivel de flexibilidad le tenemos que dar al DT inicial para que el ensemble funcione bien.\n",
    "\n",
    "***\n",
    "**Ejercicio**: probemos después otros algoritmos de base, como el `ExtraTreeClassifier`. ¿Qué ventaja hay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar el poder de boosting, hagamos un nuevo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, t = make_hastie_10_2(random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_val = X[2000:], X[:2000]\n",
    "t_train, t_val = t[2000:], t[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset consiste en diez features normales estándares, con los siguientes labels: \n",
    "\n",
    "`t[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1`\n",
    "\n",
    "Es decir, es una hiperesfera adentro de una hiperesfera hueca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "dd = pd.DataFrame.from_records(X_train)\n",
    "dd['target'] = t_train\n",
    "\n",
    "#ss.pairplot(dd.iloc[::20, :5], hue='target')\n",
    "#ss.pairplot(dd.iloc[::20], hue='target', vars=range(5), markers=',')\n",
    "sns.pairplot(dd.iloc[::20], hue='target', vars=range(4), markers='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como funciona `AdaBoost` en un caso así. Combinemos DTs con profundidad máxima 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100, learning_rate=1.0)\n",
    "\n",
    "clf.fit(X_train, t_train)\n",
    "print_results_boost(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente esto es mucho mejor que un predictor solo, que es casi lo mismo que nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(X_train, t_train)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nesti in [10, 20, 50, 75]:\n",
    "    clf.set_params(n_estimators=nesti)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print_results_boost(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero la pregunta que se impone es cuándo me detengo. Una forma de evitar el sobreajuste es detenerse cuando el error en el conjunto de validación llegue a un mínimo. Esta estrategia de regularización se llama *early stopping*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una serie con 1000 estimadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.set_params(n_estimators=1000)\n",
    "clf.base_estimator.set_params(max_depth=4)\n",
    "clf.fit(X_train, y_train)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora miro la exactitud en cada paso del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_val = []\n",
    "for y_val in clf.staged_predict(X_val):\n",
    "    accuracy_val.append(accuracy_score(t_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_val)\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.axvline(np.argmax(accuracy_val))\n",
    "plt.axhline(np.max(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` tiene una clase que implementa *Gradient Boosting* y se llama `GradientBoostingRegressor` y `GradientBoostingClassifier`, y aparece en el paquete `sklearn.ensemble`.\n",
    "\n",
    "Sin embargo, hay una poderosa librería, que se usa mucho, que se llama `XGBoost`, que implementa Gradient Boosting (Xtreme) de forma eficiente, y que tiene un hermoso paquete para `python`. Entre otras cosas, este paquete imita la API de `sklearn`, de manera que podemos usar la librería casi sin saber nada nuevo. Además, implementa cosas de forma fácil, como el *early stopping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb = xgboost.XGBClassifier(verbosity=1, n_estimators=1000)\n",
    "\n",
    "xgb.fit(X_train, t_train, eval_set=[(X_val, t_val)], early_stopping_rounds=5)\n",
    "y_val = xgb.predict(X_val)\n",
    "\n",
    "# \n",
    "accuracy_score(t_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "estimators = [('ridge', RidgeCV()),\n",
    "              ('lasso', LassoCV(random_state=42)),\n",
    "              ('lasso2', LassoCV(random_state=107)),\n",
    "              ('svr', SVR(C=1, gamma=1e-6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "reg = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y se ajusta como siempre, con la magia de sklearn\n",
    "reg = reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos obtener el output de cada uno con el método `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "final_layer = StackingRegressor(estimators=[('rf', RandomForestRegressor(random_state=42)),\n",
    "                                            ('gbrt', GradientBoostingRegressor(random_state=42))],\n",
    "                                final_estimator=RidgeCV())\n",
    "\n",
    "multi_layer_regressor = StackingRegressor(estimators=[('ridge', RidgeCV()),\n",
    "                                                      ('lasso', LassoCV(random_state=42)),\n",
    "                                                      ('svr', SVR(C=1, gamma=1e-6, kernel='rbf'))],\n",
    "                                          final_estimator=final_layer)\n",
    "\n",
    "multi_layer_regressor.fit(X_train, y_train)\n",
    "print('R2 score: {:.2f}'.format(multi_layer_regressor.score(X_test, y_test)))\n",
    "print(multi_layer_regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
