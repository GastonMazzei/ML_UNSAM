{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Ejercicios_04_Clasificacion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cmg31X8PLanz",
        "2boCsbd-LydN",
        "IQ-JKDjoHuEq",
        "xhLThwXOxFv0",
        "T_0S7UlL3HKF",
        "wxSRT5epJrKk"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbHGdHo_v73J"
      },
      "source": [
        "# Por si alguien corre en python2\n",
        "from __future__ import division\n",
        "\n",
        "# Preparamos todo para correr\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTXxK7PqLWKO"
      },
      "source": [
        "# Repaso Algoritmos de Clasificación:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmg31X8PLanz"
      },
      "source": [
        "## Función Discriminante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxQBwp1YL4SY"
      },
      "source": [
        "Mientras que en regresión lineal teniamos una (o muchas) variables continuas $t(\\vec{x})$, aquí sabemos que cada $\\vec{x}$ pertenece a una (única) clase $C_{k}$. El objetivo es entonces, teniendo un set de entrenamiento de inputs y etiquetas $(\\vec{x}_{i},t_{i})$, poder asignar una clase $C_{k}$ a un nuevo input $\\vec{x}$. Usualmente trabajaremos con la asignación $t_{k}=(... 1,..)^{T}$. Es decir, un vector de ceros excepto en el lugar $k$, donde hay un 1.\n",
        "\n",
        "Así como para regresión lineal podíamos tratar de obtener directamente la función discriminante, aquí podemos asignar un $y_{k,i}$ a cada $\\vec{x}_{i}$:\n",
        "\n",
        "$y_{k,i} = f(w_{0,k}+\\vec{w}^{T}_{k}\\vec{x}_{i})$\n",
        "\n",
        "Donde $k$ es la clase, $\\vec{w}_{k},w_{0,k}$ son los coeficientes a estimar por cada clase y $f$ es la función de activación, que constituye la primera diferencia con el enfoque de regresión lineal. Esta función de activación permite capturar mejor no linearidades y devolver un número en el intervalo $[0,1)$ para poder establecer un criterio de selección de clase. Cada criterio de selección dará lugar a _fronteras de decisión_. Si un punto cae en una frontera de decisión, se debe establecer un criterio de asignación de clase (usualmente se tira un número al azar).\n",
        "\n",
        "La estimación de los coeficientes $\\text{W}_{ik}=w_{i,k}$ puede hacerse de distintas maneras. Aquí vamos a ver tres criterios: **cuadrados mínimos**, **discriminante linear de Fisher y el perceptron**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBJFN13gLjWQ"
      },
      "source": [
        "### Cuadrados Mínimos\n",
        "\n",
        "Para el primer método, establecemos la asignación de la clase k-esima cuando $y_{k}>y_{j}$ para todo $j$ distinto de $k$. Con esta asignación de clase, aparecen las _fronteras de decisión_ donde $y_{k}=y_{j}$ para dos clases $kj$. Luego, minimizamos la función de error de cuadrados mínimos y obtenemos la expresión cerrada.\n",
        "\n",
        "$\\text{W}=(\\text{X}^{T}\\text{X})^{-1}\\text{X}^{T}\\text{T}$\n",
        "\n",
        "Donde $\\text{W}$ es la matriz cuya k-esima columna son los coeficientes $w_0,w_1,..,w_{D}$ de la clase k-esima, $\\text{X}$ es la matriz cuya i-esima fila consiste en los coeficientes $1,x_{1},...,x_{D}$ para la medición i-esima, $\\text{T}$ es la matriz de clases, donde la fila i-esima consistirá en ceros con excepción del lugar corresponde a la clase de la medición i-esima.\n",
        "\n",
        "En particular, para el caso de dos clases, puede utilizarse únicamente un $y$ y asignar la clase 1 si $y \\geq 0$ y la clase 2 si $y < 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FECV0LZhLl06"
      },
      "source": [
        "### Discriminante Linear de Fisher\n",
        "\n",
        "Vamos a utilizar el metodo de scikit-learn. El Bishop lo introduce de manera distinta.\n",
        "\n",
        "Si se quiere utilizar el discriminante linear de Fisher para dos clases, se define un $y=w_{0}+\\vec{w}^{T}\\vec{x}$ y se obtiene $\\vec{w}$ como:\n",
        "\n",
        "$\\text{S}^{-1}_{\\text{W}}(\\vec{\\mu}_{2}-\\vec{\\mu}_{1})$\n",
        "\n",
        "Donde $\\vec{\\mu}_{i}$ es la media de los $\\vec{x}$ pertenencientes a la clase i-esima y $\\text{S}_{w}$ es la varianza intra clase definida por\n",
        "\n",
        "$\\text{S}_{\\text{W}}=\\sum_{i \\in \\mathcal{C}_{1}}(\\vec{x}_{i}-\\vec{\\mu}_{1})(\\vec{x}_{i}-\\vec{\\mu}_{1})^{T}+\\sum_{i \\in \\mathcal{C}_{2}}(\\vec{x}_{i}-\\vec{\\mu}_{2})(\\vec{x}_{i}-\\vec{\\mu}_{2})^{T}$\n",
        "\n",
        "Una vez obtenido $\\vec{w}$, se puede obtener la _frontera de decisión_ como la recta perpendicular a $\\vec{w}$.\n",
        "\n",
        "Queda encontrar la distancia de la recta de decision al origen. Esto es decir $w_{0}$. Si la frontera de decision se pone en $y=0$\n",
        "\n",
        "$w_{0}=-\\frac{1}{2}\\vec{\\mu}_{2}^{T}\\text{S}^{-1}_{\\text{W}}\\vec{\\mu}_{2}+\\frac{1}{2}\\vec{\\mu}_{1}^{T}\\text{S}^{-1}_{\\text{W}}\\vec{\\mu}_{1}$\n",
        "\n",
        "Y si la frontera de decision se pone en algun otro $y_{0}$, se redefine el sesgo como $w'_{0}=w_{0}-y_{0}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCnAnQz_MJ7G"
      },
      "source": [
        "### Perceptrón\n",
        "\n",
        "El perceptron, al que ya vamos a volver, es otra forma de obtener una función de discriminación. Allí, utilizamos una función de activación $f(a)$ tal que $f(a) = 1.0$ si $a \\geq 0$ y $f(a) = -1.0$ si $a < 0$.\n",
        "\n",
        "Además, introducimos nuevamente funciones $\\phi_{j}$, usualmente con $\\phi_{0}=1.0$ de manera tal que \n",
        "\n",
        "$y(\\vec{x})=f(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$\n",
        "\n",
        "Para este algoritmo, tomamos las etiquetas $t = 1$ para $\\mathcal{C}_{1}$ y $t = -1$ para $\\mathcal{C}_{2}$.\n",
        "\n",
        "Para el perceptron puede derivarse un algoritmo secuencial de optimización de los $\\vec{w}$. Para una medición $\\vec{x}_{n}$, si el perceptron clasifica adecuadamente se prosigue pero si se clasifica erroneamente, se actualiza $\\vec{w}$ de la siguiente manera:\n",
        "\n",
        "$\\vec{w}^{\\tau+1} = \\vec{w}^{\\tau}+\\vec{\\phi}(\\vec{x}_{n})t_{n}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2boCsbd-LydN"
      },
      "source": [
        "## Modelo Discriminativo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU8L2YkBMmel"
      },
      "source": [
        "### Regresión Logística\n",
        "\n",
        "En lugar de buscar directamente la función discriminadora podemos tratar de modelar los posterior de nuestro clasificador obteniendo lo que se llama un _modelo discriminador probabilistico_. Uno de estos modelos es el de la Regresión Logistica.\n",
        "\n",
        "En este modelado, que veremos para dos clases posibles, para un dado $\\vec{x}$ modelamos la prioridad de que pertenezca a la clase $\\mathcal{C}_{1}$ como:\n",
        "\n",
        "$p(\\mathcal{C}_{1}|\\vec{x})=\\sigma(\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))$\n",
        "\n",
        "Con $p(\\mathcal{C}_{2}|\\vec{x})=1-p(\\mathcal{C}_{1}|\\vec{x})$. $\\sigma$ es la función sigmoide\n",
        "\n",
        "$\\sigma(a)=\\frac{1}{1+e^{-a}}$\n",
        "\n",
        "Para este modelo, si se tienen $M$ funciones fijas $\\phi$, se tienen $M$ parametros ajustables. \n",
        "\n",
        "Para un dataset $\\vec{x}_{n}$, con $n=1,..,N$, la verosimilitud es\n",
        "\n",
        "$p(\\text{T}|\\vec{w})=\\prod_{n=1}^{N}y^{t_n}_{n}(1-y_{n})^{1-t_n}$\n",
        "\n",
        "De manera tal que, maximizando la verosimilitud, podemos definir un algoritmo iterativo para encontrar los $\\vec{w}$, el Iterative Reweighted Least Squares o IRLS. Para la regresión logistica, la función de error es la cross-entropy:\n",
        "\n",
        "$E(\\vec{w})=-\\sum_{n=1}^{N}(t_{n}\\text{ln}y_{n}+(1-t_{n})\\text{ln}(1-y_{n}))$\n",
        "\n",
        "Para minimizar este error, el algoritmo utiliza una actualización de Newton-Ralphson:\n",
        "\n",
        "$\\vec{w}^{\\text{nuevo}}=\\vec{w}^{\\text{viejo}}-\\text{H}^{-1}\\nabla E(\\vec{w})$\n",
        "\n",
        "Donde $\\nabla E(\\vec{w})$ es el gradiente del error y $\\text{H}$ es la matriz Hessiana. Para la regresion logistica, uno puede llegar a sus propias ecuaciones normales _iterativas_\n",
        "\n",
        "$\\vec{w}^{\\text{nuevo}}=(\\text{$\\Phi$}^{T}\\text{R}\\text{$\\Phi$})^{-1}\\text{$\\Phi$}^{T}\\text{R}\\text{z}$\n",
        "\n",
        "Con $\\Phi$ la matriz de disenio, $\\text{R}$ la matriz diagonal cuyos elementos son $y_{n}(1-y_{n})$ y $\\text{z}$ es un vector que se calcula como:\n",
        "\n",
        "$\\text{z}=\\text{$\\Phi$}\\vec{w}^{\\text{viejo}}-\\text{R}^{-1}(\\text{Y}-\\text{T})$\n",
        "\n",
        "Con $\\text{Y}$ e $\\text{T}$ los vectores de predicciones y respuestas respectivamente. Noten que los pesos entran varias veces: en la matriz $\\text{R}$, en el vector $\\text{Y}$ y explicitamente en $\\text{z}$. Es por esto que es iterativo.\n",
        "\n",
        "\n",
        "Este algoritmo también puede aplicarse al caso de regresión lineal, viendo que el algoritmo de Newton-Raphson converge a la solución cerrada en 1 paso. La diferencia aquí es la función de activación sigmoide. Además, quiero enfatizar que este algoritmo es iterativo pero no es secuencial, ya que utiliza todos los datos del dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ-JKDjoHuEq"
      },
      "source": [
        "# Ejercicio 1: Clasificación con función discriminante\n",
        " Clasifique los siguientes Datasets utilizando función discriminante optimizada por cuadrados mínimos y por el discriminante lineal de Fisher. Encuentre los coeficientes $\\text{W}$ para el caso de dos clases, con la frontera de decisión y la tasa de misclasificación con ambos métodos. Para esto último, utilice el método de K-Folding.\n",
        "\n",
        "Discuta que diferencia hay entre ambos datasets y como impacto esto a la performance de los algoritmos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ9QNfo39sGD"
      },
      "source": [
        "**a.** Dataset 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nfeIh5g2hv8"
      },
      "source": [
        "from scipy.stats import  multivariate_normal\n",
        "X1=multivariate_normal.rvs(size=100,mean=[0,7],cov=[[1,0.0],[0.0, 1]])\n",
        "X2=multivariate_normal.rvs(size=100,mean=[2,4],cov=[[2,0.0],[0.0, 1]])\n",
        "plt.scatter(X1[:,0],X1[:,1],color='blue')\n",
        "plt.scatter(X2[:,0],X2[:,1],color='red')\n",
        "plt.xlim((-3,10))\n",
        "plt.ylim((-1,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP15Rz3Gbm3D"
      },
      "source": [
        "**b.** Dataset 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNu-Cfp6an59"
      },
      "source": [
        "from scipy.stats import  multivariate_normal\n",
        "X1=multivariate_normal.rvs(size=100,mean=[0,7],cov=[[1,0.0],[0.0, 1]])\n",
        "aux1=multivariate_normal.rvs(size=90,mean=[2,4],cov=[[2,0.0],[0.0, 1]])\n",
        "aux2=multivariate_normal.rvs(size=10,mean=[9,0],cov=[[0.2,0.0],[0.0, 0.1]])\n",
        "X2=np.zeros(X1.shape)\n",
        "X2[:,0]=np.append(aux1[:,0],aux2[:,0])\n",
        "X2[:,1]=np.append(aux1[:,1],aux2[:,1])\n",
        "plt.scatter(X1[:,0],X1[:,1],color='blue')\n",
        "plt.scatter(X2[:,0],X2[:,1],color='red')\n",
        "plt.xlim((-5,10))\n",
        "plt.ylim((-1,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CpsF9VJ-LBM"
      },
      "source": [
        "**c.** Repita la clasificación en ambos Datasets utilizando la implementación provista en la clase ``sklearn.discriminant_analysis.LinearDiscriminantAnalysis()``\n",
        "\n",
        "NOTA: Para utilizar el discriminante lineal de Fisher, utilice `` solver=‘eigen’``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOE9ANp4_y7W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLThwXOxFv0"
      },
      "source": [
        "# Ejercicio 2: El Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1G7qPuoAIrC"
      },
      "source": [
        "**a.** \n",
        "Utilice un perceptron para realizar clasificación en el primer dataset del ejercicio anterior y vea si consigue una buena convergencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfYbtiRN3BQM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IF_qrxgAZwy"
      },
      "source": [
        "**b.** Repita el ejercicio haciendo uso de la implementación provista en la clase ``sklearn.linear_model.Perceptron()``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5sQTfUvATUL"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_0S7UlL3HKF"
      },
      "source": [
        "# Ejercicio 3: Regresión Logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uz8PEGRA4T4"
      },
      "source": [
        "**a.** Aplique clasificación por Regresión Logística a los dos datasets del Ejercicio 1.\n",
        "\n",
        "*Nota:* Para favorecer la convergencia sin problemas numéricos, utilice como valor inicial de $\\vec{w}$ el resultado de alguno de los algoritmos previos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuHv9kXp3Iue"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMxnHnhUBnM5"
      },
      "source": [
        "**b.** Repita el ejercicio haciendo uso de la implementación provista en la clase ``sklearn.linear_model.LogisticRegression()`` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQRPRSv6BmUW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxSRT5epJrKk"
      },
      "source": [
        "# Ejercicio 4: Funciones de base\n",
        "\n",
        "Veamos un caso en el que los datos son en apariencia no linealmente separables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SThq_jImKWv0"
      },
      "source": [
        "phi=-np.pi+2*np.pi*np.random.rand(100)\n",
        "L=int(len(phi)/2)\n",
        "r1=np.random.rand(L)\n",
        "r2=1.0+2.0*np.random.rand(L)\n",
        "T=np.zeros(2*L)\n",
        "X=np.zeros((2*L,2))\n",
        "for i in range(L):\n",
        "  X[i,0]=r1[i]*np.cos(phi[i])\n",
        "  X[i,1]=r1[i]*np.sin(phi[i])\n",
        "  T[i]=1.0\n",
        "  X[L+i,0]=r2[i]*np.cos(phi[L+i])\n",
        "  X[L+i,1]=r2[i]*np.sin(phi[L+i])\n",
        "plt.scatter(X[:L,0],X[:L,1],color='blue')\n",
        "plt.scatter(X[L:,0],X[L:,1],color='red')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgCtF_3_KXzR"
      },
      "source": [
        "**a** Trate de clasificar los datos utilizando Regresion Logistica como en los ejercicios anteriores. Obtenga la matriz de confusion. Que observa?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLOCQ7JCKn2n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx-PGN_lKoFz"
      },
      "source": [
        "**b** Haga el cambio de base a coordenadas polares:\n",
        "\n",
        "$r = \\sqrt{x^{2}_{1}+x^{2}_{2}}$\n",
        "\n",
        "$\\phi = \\text{Arctan}\\left(\\frac{x_{2}}{x_{1}}\\right)$\n",
        "\n",
        "Para lo segundo, utilize la funcion `np.arctan2`. Grafique y entrene un Regresor Logistico en esta nueva base. Que encuentrea?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXRwiyvNLZOo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btfj-w0TLYka"
      },
      "source": [
        "**c** Grafique la frontera de decision en la base original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4TIyulvLee4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrhxOKYsWhJ"
      },
      "source": [
        "# Ejercicio 5: Dataset de Iris (Multiclase):\n",
        "\n",
        "A continuación utilizaremos el famoso dataset de Iris, tomado del paper de Fisher del 1936 en donde lo utilizó por primera vez para ejmemplificar el uso del análisis determinante lineal. \n",
        "\n",
        "El dataset consiste de 150 muestras de 3 tipos distintos de flor Iris, 50 muestras de cada una. Cada muestra consiste de las medidas (en cm) del ancho y largo del pétalo y sépalo de una flor. \n",
        "\n",
        "Su trabajo es entrenar un modelo que correctamente clasifique de qué tipo es una flor Iris, dadas sus medidas de ancho y largo de sus pétalos y sétalos. Para esto, explore y juegue con los modelos vistos hasta ahora. Cuando termine, de una estimación objetiva de la ``accuracy`` esperada de su modelo. \n",
        "\n",
        "*Bonus: intente identificar si entre las tres clases alguna o todas son o no linealmente separables.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ci5rsoStDjl"
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ifX_ekXFpX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU7FpuL_XEpS"
      },
      "source": [
        "Ahora suponga que en que usted es alergico a una de las tres flores, la Setosa. Utilice una matriz de perdida sobre el mejor modelo que tenga y obtenga una nueva matriz de confusion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3PNrN7cRk6P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}