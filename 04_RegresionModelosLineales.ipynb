{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Lineales - Parte 1. Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"05_ModelosLineales\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal (univariada)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vuelve a llamar Alex. Está agradecido. Pudo impresionar a su jefe gracias a lo que le mandamos de Random Forests, Neural Networks, etc. Lo nombraron *Chief Artificial Intelligence Officer*, a cargo de departamento de AI de la inmobiliaria. Pero está de vuelta en problemas (si no, creo que no llamaría). \n",
    "\n",
    "Resulta que hace un par de fin de semanas, el jefe de Alex estaba jugando al golf con otros dueños de inmobiliarias, y se mandó la parte con su recientemente creado departamento de Inteligencia Artificial, y les contó todo lo que habían logrado (gracias a nosotrxs) con respecto a la predicción de precios de casas en distritos de California. Pero se vio en un brete cuando uno de sus colegas le pidió alguna explicación respecto al funcionamiento de la predicción. ¿Qué variables son las más importantes? ¿Cómo hace la predicción?\n",
    "\n",
    "El lunes siguiente, el jefe volvió a la inmobiliaria y entró precipitadamente en la oficina de Alex para pedirle que arme un equipo que pudiera darle algún sentido a las predicciones que habían logrado antes. Le dio permiso para contratar a una persona. Alex está contento porque logró meter a su novia, Brenda, como *Senior Parameter Explorer*, pero tienen que producir algún modelo que sea más fácil de entender para el jefe, sobre todo para que no se deschave que Brenda en realidad estudio diseño gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvamos a leer los datos de California\n",
    "HOUSING_PATH = os.path.join(\".\", \"datasets\", \"housing\")\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    \n",
    "    import tarfile\n",
    "\n",
    "    DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/IAI-UNSAM/ML_UNSAM/master/\"\n",
    "    HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "    !mkdir -p ./datasets/housing\n",
    "\n",
    "    def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "        os.makedirs(housing_path, exist_ok=True)\n",
    "        tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "        #urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "        !wget https://raw.githubusercontent.com/IAI-UNSAM/ML_UNSAM/master/datasets/housing/housing.tgz -P {housing_path}\n",
    "        housing_tgz = tarfile.open(tgz_path)\n",
    "        housing_tgz.extractall(path=housing_path)\n",
    "        housing_tgz.close()\n",
    "    \n",
    "    # Corramos la función\n",
    "    fetch_housing_data()\n",
    "    \n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente de Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a quedarnos solo con la mediana del ingreso y el valor de las casas.\n",
    "x = housing.median_income\n",
    "y = housing.median_house_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nos podemos preguntar de dónde sale ese valor de 0.688. Ya lo mencionamos, pero ahora que sabemos algo más de distribuciones, podemos volver a verlo.\n",
    "\n",
    "Si usamos la siguiente notación:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mu_X &=& \\mathbb{E}(X)\\\\\n",
    "\\sigma^2_X &=& \\mathbb{E}\\left[(X - \\mathbb{E}(X))^2\\right] = \\mathrm{var}(X)\\\\\n",
    "\\text{cov}_{XY} &=& \\mathbb{E}\\left[(X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y)\\right]\\;\\;,\n",
    "\\end{array}\n",
    "$$\n",
    "el coeficiente de correlación se define como \n",
    "\n",
    "$$\n",
    "\\rho_{XY} = \\mathbb{E}\\left[\\left(\\frac{X - \\mu_X}{\\sigma_X}\\right)\\left(\\frac{Y - \\mu_Y}{\\sigma_Y}\\right)\\right]\\;\\;.\n",
    "$$\n",
    "\n",
    "El valor de expectación $\\mathbb{E}$ lo definimos para las distribuciones hace unas clases. Para el caso continuo:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(X) = \\int x f_X(x) \\mathrm{d}x\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, ¿qué pasa si no conocemos, como ahora, $f_X$? ¿Qué pasa si solo tenemos una muestra de esa distribución?\n",
    "\n",
    "Entonces, tenemos que usar los datos para *estimar* el valor esperado, $\\mathbb{E}(X)$. Para eso, formamos un *estadístico* (*statistic*, en singular, en inglés). Es decir, una función de los datos.\n",
    "\n",
    "$$\n",
    "\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N x_i\\;\\;.\n",
    "$$\n",
    "\n",
    "Cuando un estadístico se usa para aproximar el valor de una característica poblacional no conocido, se lo llama un *estimador*. En este caso el estadístico $\\bar{X}$ es un *estimador* del valor de expectación de la distribución, y a veces vamos a usar la notación:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu_X} = \\bar{X}\\;\\;.\n",
    "$$\n",
    "\n",
    "También tenemos *estimadores* de la varianza y de la covarianza:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_X^2 = \\frac{1}{N - 1}\\sum_{i=1}^N (x_i - \\bar{X})^2\\;\\;,\n",
    "$$\n",
    "y\n",
    "$$\n",
    "\\hat{\\mathrm{cov}}_{XY} = \\frac{1}{N - 1}\\sum_{i=1}^N (x_i - \\bar{X})(y_i - \\bar{Y})\\;\\;.\n",
    "$$\n",
    "\n",
    "Con todo esto, podemos calcular un estimador del coeficiente de correlación:\n",
    "\n",
    "$$\n",
    "\\hat{\\rho_{XY}} = r = \\frac{\\hat{\\mathrm{cov}}_{XY}}{\\hat{\\sigma}_X \\hat{\\sigma}_Y}\\;\\;,\n",
    "$$\n",
    "que se conoce como el *coeficiente de correlación de Pearson*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los estimadores de los valores medios\n",
    "Xbar = np.sum(x)/len(x)\n",
    "Ybar = np.sum(y)/len(y)\n",
    "\n",
    "# Calculamos los estimadores de los desvíos estándar (es decir, sqrt(cov))\n",
    "sigmaX = np.sqrt(np.sum((x - Xbar)**2) / (len(x) - 1))\n",
    "sigmaY = np.sqrt(np.sum((y - Ybar)**2) / (len(y) - 1))\n",
    "\n",
    "covXY = np.sum((x - Xbar) * (y - Ybar)) / (len(x) - 1)\n",
    "\n",
    "# Coeficiente de Pearson\n",
    "r = covXY / (sigmaX * sigmaY)\n",
    "\n",
    "print('El coeficiente de Pearson es: {:.3f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `numpy` tenemos funciones y métodos que hacen esto: `np.mean`, `np.std`, `np.cov`, `np.corrcoef`.\n",
    "\n",
    "De ahora en más, usen esas implementaciones, porque en el fondo, `numpy` corre código en C y es mucho más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El coeficiente de correlación toma valores $-1 < \\rho < 1$, donde -1 indica una anticorrelación perfecta, y 1 indica una correlación perfecta.\n",
    "\n",
    "El estimador, $r$, es muy sensible a puntos aberrantes (*outliers*), por lo que hay que mirar un poco los datos antes de calcular los coeficientes y ya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "scipy.stats.spearmanr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '.', alpha=0.1)\n",
    "plt.axhline(y.mean(), color='r')\n",
    "plt.axvline(x.mean(), color='r')\n",
    "plt.xlabel('Mediana del ingreso por distrito')\n",
    "plt.ylabel('Mediana del precio de la casa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos como cambia $r$ si sacamos los distritos con precios que saturan\n",
    "i = y < np.max(y)\n",
    "\n",
    "plt.plot(x[i], y[i], '.', alpha=0.1)\n",
    "plt.axhline(y[i].mean(), color='r')\n",
    "plt.axvline(x[i].mean(), color='r')\n",
    "plt.xlabel('Mediana del ingreso por distrito')\n",
    "plt.ylabel('Mediana del precio de la casa')\n",
    "\n",
    "\n",
    "r = np.corrcoef(x[i], y[i])[0, 1]\n",
    "print('El coeficiente de Pearson, con datos corregidos, es: {:.8f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro. Los puntos saturados, arriba a la derecha ayudan mucho a aumentar $r$ de forma espúrea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = x < 10\n",
    "\n",
    "plt.plot(x[i & j], y[i & j], '.', alpha=0.1)\n",
    "plt.axhline(y[i & j].mean(), color='r')\n",
    "plt.axvline(x[i & j].mean(), color='r')\n",
    "plt.xlabel('Mediana del ingreso por distrito')\n",
    "plt.ylabel('Mediana del precio de la casa')\n",
    "\n",
    "r = np.corrcoef(x[i & j], y[i & j])[0, 1]\n",
    "print('El coeficiente de Pearson, con datos corregidos, es: {:.8f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo lineal sencillo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ingreso no es una variable ideal para determinar el precio de una casa, pero es lo mejor que tenemos, de manera individual. ¿Qué modelo podemos proponerles?\n",
    "\n",
    "Brenda recordó algo de algunas clases de matemática que tuvo en la carrera y pensó que podía armar un modelo que tuviera esta pinta:\n",
    "\n",
    "$$\n",
    "y = a * x + b\\;\\;,\n",
    "$$\n",
    "es decir, una relación lineal entre ambas variables. En este tipo de modelo, la variable $x$ se conoce con el nombre de variable predictora. Este modelo tiene dos *parámetros*: $a$ y $b$, la pendiente y ordenada al origen, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brenda encuentra en sus apuntes viejos una expresión para *ajustar* los parámetros:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\hat{a} &=& \\sum_{i=0}^N (x_i - \\bar{X}) (y_i - \\bar{Y}) \\left[\\sum_{i=0}^N (x_i - \\bar{X})^2\\right]^{-1}\\\\\n",
    "\\hat{b} &=& \\bar{Y} - \\hat{a}\\bar{X}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Comparando con las ecuaciones de arriba, vemos que\n",
    "\n",
    "$$\n",
    "\\hat{a} = \\frac{\\hat{\\mathrm{cov}}_{XY}}{\\hat{\\mathrm{var}}(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos\n",
    "ahat = np.cov(x[i], y[i])[0,1] / np.var(x[i])\n",
    "bhat = np.mean(y[i]) - ahat * np.mean(x[i])\n",
    "\n",
    "print('Los ajustes dan')\n",
    "print('a = {:.3f}'.format(ahat))\n",
    "print('b = {:.3f}'.format(bhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[i], y[i], '.', alpha=0.1)\n",
    "\n",
    "xx = np.linspace(x[i].min(), x[i].max(), 3)\n",
    "plt.plot(xx, xx * ahat + bhat, 'r-')\n",
    "plt.xlabel('Mediana del ingreso por distrito')\n",
    "plt.ylabel('Mediana del precio de la casa [$]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver los residuos, y calcular su dispersión\n",
    "res = y[i] - (ahat * x[i] + bhat)\n",
    "\n",
    "plt.plot(x[i], res, '.', alpha=0.1)\n",
    "plt.axhline(0, color='0.2', ls=':')\n",
    "plt.xlabel('Mediana del ingreso por distrito')\n",
    "plt.ylabel('Residuos [O - C]')\n",
    "\n",
    "\n",
    "print('Los residuos tienen una dispersión de $ {:.3f}'.format(res.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**\n",
    "\n",
    "- ¿Estamos contentos con el ajuste? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿De dónde viene la fórmula de regresión lineal? (o ¿qué condiciones se tienen que cumplir para que la cosa funcione?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formalizando un poco más la cosa, podemos recordar que habíamos dicho que un modelo podía escribirse:\n",
    "\n",
    "$$\n",
    "y_i = m_i + \\epsilon_i\\;\\;,\n",
    "$$\n",
    "donde $\\epsilon_i$ es el término de error.\n",
    "\n",
    "De ahora en más, vamos a llamar:\n",
    "* $t_i$ a los valores de $Y$ que queremos reproducir,\n",
    "* $y(x, \\omega)$ al modelo, que dependerá de una vector de parámetros $\\boldsymbol{\\omega}$.\n",
    "\n",
    "En este caso, estamos tomando, $y(x, \\boldsymbol{\\omega}) = \\omega_0 + \\omega_1 * x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que los errores $\\epsilon$ tienen media cero, $\\mathbb{E}(\\epsilon)=0$, y que están distribuidos de acuerdo a una normal (Gaussiana), y que tienen la misma varianza $\\beta^{-1}$ **para todos los puntos** (ya veremos por qué usamos la inversa):\n",
    "\n",
    "$$\n",
    "\\epsilon_i \\sim \\mathcal{N}(0, \\beta^{-1})\\;\\; \\text{para } i = \\left\\{1, \\ldots, N\\right\\}\n",
    "$$\n",
    "\n",
    "En general, podemos pensar que el objetivo es encontrar el $y$ que mejor reproduce los $t_i$. \n",
    "\n",
    "Pero en términos bayesianos, podemos decir que buscamos conocer la distribución de probabilidad de las variables $t_i$, condicionada al valor correspondiente $x_i$. Bajo las hipótesis de arriba:\n",
    "\n",
    "$$\n",
    "p(t | x, \\boldsymbol{\\omega}, \\beta) = \\mathcal{N}(t | y(x, \\boldsymbol{\\omega}), \\beta^{-1})\\;\\;.\n",
    "$$\n",
    "\n",
    "Noten que esta expresión corresponde a la verosimilitud del dato $t_i$. Una criterio para encontrar los parámetros del vector $\\omega$ es maximizar la verosimilitud.\n",
    "\n",
    "Si suponemos además que los datos son independientes, tenemos:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{t} | \\mathbf{x}, \\boldsymbol{\\omega}, \\beta) = \\prod_{i=1}^N \\mathcal{N}(t_i | y(x_i, \\boldsymbol{\\omega}), \\beta^{-1})\\;\\;,\n",
    "$$\n",
    "que es la verosimilitud de nuestro problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, es más conveniente usar el logaritmo de la verosimilitud. Como el logaritmo es una función creciente, el máximo de la verosimilitud coincide con el máximo de su logaritmo.\n",
    "\n",
    "En este caso, obtenemos:\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{t} | \\mathbf{x}, \\boldsymbol{\\omega}, \\beta) = -\\frac{\\beta}{2} \\sum_{i=1}^{N} \\left\\{\\left(y(x_i, \\boldsymbol{\\omega}) - t_i\\right)^2\\right\\} + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln 2\\pi\\;\\;.\n",
    "$$\n",
    "\n",
    "Como los últimos dos términos son constantes con respecto a los parámetros, podemos obviarlos a la hora de maximizar la verosimilitud. La tarea es, entonces, equivalente a minimizar:\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{\\omega}) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\\;\\;,\n",
    "$$\n",
    "que se conoce como *error cuadrático medio*.\n",
    "\n",
    "Muchas veces, es más simpático tener una función de error que tenga las mismas unidades que los datos. Por eso, definimos la *raíz del error cuadrático medio* (*root-mean-square error*, o *RMS*):\n",
    "\n",
    "$$\n",
    "E_{RMS} = \\sqrt{2 * E(\\boldsymbol{\\omega})/N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Mostrar usando la expresión del error cuadrático medio, y el modelo lineal, que los valores de los parámetros $a$ y $b$ que maximizan la verosimilitud son los que aparecen arriba.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Nuevo</font>**. Usando estas expresiones, pero derivando con respecto a $\\beta$, podemos mostrar que:\n",
    "$$\n",
    "\\frac{1}{\\beta_{ML}} = \\frac{1}{N}\\sum_{i=1}^N\\left\\{y(x_i, \\boldsymbol{\\omega}_{ML}) - t_i\\right\\}^2\n",
    "$$\n",
    "\n",
    "¡Ojo! Esta es la estimación de la varianza de los errores, no de la de los residuos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Demostrar la expresión del estimador de máxima verosimilitud de la precisión $\\beta$, derivando el logaritmo de la verosimilitud con respecto a $\\beta$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de las hipótesis\n",
    "\n",
    "Volvamos al caso de California y estudiemos con más detalle lo que pasa con los residuos.\n",
    "\n",
    "En primer lugar, podemos calcular el error cuadrático medio (y su raíz) para el ajuste que hicimos arriba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 0.5 * np.sum(res**2)\n",
    "\n",
    "res0 = res.copy()\n",
    "print('El error cuadrático medio es {:.3f}'.format(E))\n",
    "print('El root-mean-square error es ${:.3f}'.format(np.sqrt(2 * E / len(res))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invbeta = np.sum(res0**2) / len(res0)\n",
    "print(np.sqrt(invbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Pregunta**\n",
    "\n",
    "* ¿Cómo se les ocurre que podemos estudiar el cumplimiento de las hipótesis?\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos la distribución de los residuos\n",
    "a = plt.hist(res, 100, density=True)\n",
    "\n",
    "# Podemos graficar una Gaussiana encima\n",
    "# Vamos a plotear la _mejor_\n",
    "xx = np.linspace(res.min(), res.max(), 100)\n",
    "plt.plot(xx, st.norm.pdf(xx, res.mean(), res.std()), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Diagrama Q-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Diagrama qq\n",
    "import scipy.stats as st\n",
    "# st.probplot(res, plot=plt.gca())\n",
    "st.probplot((res - res.mean())/res.std(), plot=plt.gca(), fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# En un caso que sí funciona\n",
    "mm = st.norm.rvs(0, 1, size=5000)\n",
    "# plt.hist(mm, 20)\n",
    "a = st.probplot(mm, plot=plt.gca(), fit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Plot de violín y de caja (violin / box plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.histogram(x[i], 25)\n",
    "\n",
    "bb = []\n",
    "\n",
    "for k in range(len(a[0])):\n",
    "    if a[0][k] < 100:\n",
    "        continue\n",
    "    \n",
    "    cond = (x[i] >= a[1][k]) & (x[i] < a[1][k+1])\n",
    "    bb.append(res[cond].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot(111)\n",
    "m = [bbb.mean() for bbb in bb]\n",
    "s = [bbb.std() for bbb in bb]\n",
    "plt.errorbar(np.arange(len(bb)), m, s, fmt='or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "vv = plt.boxplot(bb, whis=[5, 95], showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "vv = plt.violinplot(bb, showmeans=True, showextrema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal con datos sintéticos (o el fin de la era de inferencia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos arriba que los datos de California no parecen estar verificando las hipótesis de la regresión lineal.\n",
    "\n",
    "Vale entonces preguntarse:\n",
    "\n",
    "* ¿Qué pasa en este caso?\n",
    "\n",
    "* ¿A qué riesgos me expongo si no se verifican las hipótesis?\n",
    "\n",
    "Para respoder eso, hagamos primero un aparte para discutir las propiedades del estimador de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Aparte: Estimador de máxima verosimilitud (ML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cuando obtenemos el estimador $\\hat\\omega_0$, y $\\hat\\omega_1$ a partir de la minimización de la función de error cuadrática (o la maximización del logaritmo de la verosimilitud), se dice que se trata de estimadores de máxima verosimilitud.\n",
    "\n",
    "Los estimadores de máxima verosimilitud tienen una propiedades muy convenientes para una gran número de muestras (no está garantizado en el caso un tamaño finito de muestras):\n",
    "\n",
    "* **Invarianza frente a transformaciones del parámetro**. Esto quiere decir que si cambiamos la parametrización del problema, por ejempo, pasamos de $y(x, \\boldsymbol{\\omega}) = \\omega_0 + \\omega_1 * x$, a $y(x, \\omega_0 , \\tau(\\omega_1)) = \\omega_0 + \\tau * x$ (por ejemplo, un cambio de variables de $x$), entonces $\\hat{\\tau(\\omega_1)} = \\tau(\\hat\\omega_1)$.\n",
    "\n",
    "\n",
    "* **Consistencia**. A medida que aumenta el número de datos, el estimador de ML converge hacia los \"verdaderos\" valores de los parámetros.\n",
    "\n",
    "\n",
    "* **Eficiencia**. El estimador de ML alcanza la mínima varianza que puede tener un estimador no sesgado (límite de Cramér-Rao).\n",
    "\n",
    "\n",
    "* **Normalidad**. Asintóticamente, el estimador de ML tiene una distribución normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Aparte II: covarianza de los estimadores y predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ahora, podemos preguntarnos por la varianza de un estimador, y cómo se calcula.\n",
    "\n",
    "Bajo algunas condiciones, más o menos generales, podemos mostrar (ver Frodesen, Cap. 9) que el estimador de máxima verosimilitud, en el limite de muchos datos, está distribuído como una distribución multinormal, centrada en los verdaderos valores y con matriz de covarianza, $\\boldsymbol{\\Sigma}$, cuya inversa es:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}^{-1}_{ij} = -\\left. \\frac{\\partial^2 \\ln \\mathcal{L}}{\\partial \\omega_i \\omega_j} \\right\\rvert_{\\boldsymbol{\\omega} = \\boldsymbol{\\omega_{ML}}}\\;\\;.\n",
    "$$\n",
    "\n",
    "Ufff, no se desanimen. Lo único que dice esto es que para encontrar los elementos de la matriz de covarianza, tenemos que derivar la función de error con respecto a los parámetros y evaluar en los estimadores de máxima verosimilitud. Luego, invertir la matriz resultante.\n",
    "\n",
    "En el caso de la regresión lineal, esto da:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\frac{\\beta^{-1}}{\\sum_{i=1}^N(x_i - \\bar{x})^2}\\begin{pmatrix}\\frac{1}{N}\\sum_{i=1}^N x_i^2 & -\\bar{x}\\\\\n",
    "-\\bar{x} & 1\\\\\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "O sea:\n",
    "\n",
    "$$\n",
    "\\mathrm{var}(\\omega_0) = \\frac{1}{N\\beta}\\frac{\\sum_{i=1}^N x_i^2}{\\sum_{i=1}^N(x_i - \\bar{x})^2}\n",
    "$$\n",
    "$$\n",
    "\\mathrm{var}(\\omega_1) = \\frac{1}{\\beta}\\frac{1}{\\sum_{i=1}^N(x_i - \\bar{x})^2}\\\\\n",
    "$$\n",
    "$$\n",
    "\\mathrm{cov}(\\omega_0, \\omega_1) = -\\frac{1}{\\beta}\\frac{\\bar x}{\\sum_{i=1}^N(x_i - \\bar{x})^2}\\\\\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Entonces, cuando calculemos las predicciones de nuestro modelo, ahora podemos asignarle también un error. Podemos hacer propagación de errorres\n",
    "\n",
    "$$\n",
    "t' = \\hat\\omega_0 + \\hat\\omega_1 \\cdot x\\;\\;,\n",
    "$$\n",
    "de lo que se deduce:\n",
    "\n",
    "$$\n",
    "\\mathrm{var}(t') = \\beta + \\Sigma_{00} + \\Sigma_{11} \\cdot x^2 + 2 \\Sigma_{01} \\cdot x\\;\\;.\n",
    "$$\n",
    "\n",
    "Cuando hacemos la predicción, ahora podemos también dar un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Definimos dos funciones útiles\n",
    "def compute_covariance(x, beta):\n",
    "    s = np.sum((x - x.mean())**2)\n",
    "    return np.array([[np.sum(x**2)/len(x), -x.mean()],[-x.mean(), 1]]) / s / beta\n",
    "\n",
    "def var_estimation(x, sigma_):\n",
    "    return sigma_[0, 0] + sigma_[1, 1] * x**2 + 2 * sigma_[0, 1] * x\n",
    "\n",
    "def var_prediction(x, sigma_, beta):\n",
    "    return 1/beta + sigma_[0, 0] + sigma_[1, 1] * x**2 + 2 * sigma_[0, 1] * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compute covariance\n",
    "sigma_ = compute_covariance(x[i], 1/invbeta)\n",
    "\n",
    "# Compute prediction variance\n",
    "xx = np.linspace(x[i].min(), x[i].max(), 100)\n",
    "tprime = xx * ahat + bhat\n",
    "vtprime = var_prediction(xx, sigma_, 1/invbeta)\n",
    "vestimation = var_estimation(xx, sigma_)\n",
    "\n",
    "# Las usamos para las predicciones\n",
    "plt.plot(x[i], y[i], '.', alpha=0.1)\n",
    "plt.plot(xx, tprime, 'r-')\n",
    "plt.plot(xx, tprime - np.sqrt(vtprime), 'r:')\n",
    "plt.plot(xx, tprime + np.sqrt(vtprime), 'r:')\n",
    "\n",
    "plt.plot(xx, tprime - 5*np.sqrt(vestimation), color='Brown')\n",
    "plt.plot(xx, tprime + 5*np.sqrt(vestimation), color='Brown')\n",
    "\n",
    "plt.xlabel('Mediana del ingreso por distrito')\n",
    "plt.ylabel('Mediana del precio de la casa [$]')\n",
    "\n",
    "#plt.xlim(0, 10)\n",
    "#plt.ylim(0, 400000)\n",
    "plt.axvline(x[i].mean(), color='0.5', alpha=0.5)\n",
    "plt.axhline(y[i].mean(), color='0.5', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vemos dos conceptos diferentes de error, con sus respectivos errores. Por un lado, el error en la determinación de la recta, con errores mucho menores. Es decir, si el modelo fuera correcto, cuán bien conoceríamos la recta. Por otro lado, el error de predicción, que también incluye un término relacionado con la varianza de la distribución de los datos.\n",
    "\n",
    "Vamos a usar estas ideas para estudiar el efecto de las hipótesis en la determinación de los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos sintéticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar sets de datos que violen las hipótesis de la regresión lineal. A saber:\n",
    "\n",
    "1. Errores independientes.\n",
    "\n",
    "2. Errores distribuidos como una normal.\n",
    "\n",
    "3. Homoscedasticidad. Todos los términos de error tienen al misma varianza, que habíamos llamado $\\beta^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construyamos primero el modelo verdadero.\n",
    "import numpy.random as rr\n",
    "\n",
    "rr.seed(20200831)\n",
    "\n",
    "# Parámetros de la ground truth\n",
    "b = 4\n",
    "m = 5\n",
    "beta = 1./1.\n",
    "\n",
    "# Número de datos\n",
    "n = 100\n",
    "\n",
    "# Variable predictora\n",
    "x_ = 2 * np.random.rand(n, 1)\n",
    "\n",
    "# El modelo real (ground truth)\n",
    "t_ = b + m * x_\n",
    "\n",
    "# Array para plots\n",
    "xx = np.linspace(x_.min(), x_.max(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones útiles\n",
    "\n",
    "def plot_data_truth(t):\n",
    "    plt.plot(x_, t, 'or')\n",
    "    plt.plot(xx, xx * m + b, 'k-', lw=2, alpha=0.7, label='Ground Truth')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.legend(loc=0)\n",
    "    return\n",
    "\n",
    "def see_results(x_, target_, title, prediction=False):\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "\n",
    "    # Ajustamos\n",
    "    lr.fit(x_, target_)\n",
    "\n",
    "    # Y pedimos los coeficientes\n",
    "    print('$\\hat\\omega0$ = ',lr.intercept_[0])\n",
    "    print('$\\hat\\omega1$ = ',lr.coef_[0, 0])\n",
    "\n",
    "    # Verificamos que da lo mismo\n",
    "    omega1 = np.sum((x_ - x_.mean()) * (target_ - target_.mean())) / np.sum((x_ - x_.mean())**2)\n",
    "    omega0 = target_.mean() - omega1 * x_.mean()\n",
    "    print(omega0, omega1)\n",
    "    \n",
    "    # Compute residuals\n",
    "    res = x_ * lr.coef_[0][0] + lr.intercept_[0] - target_\n",
    "    \n",
    "    # Compute oversampled estimate curve\n",
    "    tml = xx * lr.coef_[0][0] + lr.intercept_[0]\n",
    "\n",
    "    # Estimate beta\n",
    "    invbeta = np.sum(res**2)/len(res)\n",
    "    \n",
    "    # Calculemos ahora el error\n",
    "    sigma_ = compute_covariance(x_, 1.0)\n",
    "    vesti = var_estimation(xx, sigma_)\n",
    "    \n",
    "    # Plot \n",
    "    plot_data_truth(target_)\n",
    "    # add estimate curve\n",
    "    plt.plot(xx, tml, '-g', label='ML Fit', lw=3, alpha=0.5)\n",
    "    # add errors\n",
    "    plt.plot(xx, tml - 2 * np.sqrt(vesti0), ':b')\n",
    "    plt.plot(xx, tml + 2 * np.sqrt(vesti0), ':b')\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(loc=0)\n",
    "    # title\n",
    "    plt.title(title)\n",
    "    \n",
    "    fig1 = plt.gcf()\n",
    "    \n",
    "    # Veamos los residuos\n",
    "    plt.figure()\n",
    "    plt.plot(x_, target_ - (x_ * lr.coef_[0][0] + lr.intercept_[0]), 'or')\n",
    "    plt.axhline(0.0, color='g', lw=4, alpha=0.5)\n",
    "    plt.plot(xx, 2 * np.sqrt(vesti), ':b')\n",
    "    plt.plot(xx, -2 * np.sqrt(vesti), ':b')\n",
    "    \n",
    "    if prediction:\n",
    "        vpredi = var_prediction(xx, sigma_, 1/invbeta)\n",
    "        plt.plot(xx, np.sqrt(vpredi),  ls=':', color='Brown')\n",
    "        plt.plot(xx, -np.sqrt(vpredi), ls=':', color='Brown', label='predition error (1sigma)')\n",
    "        plt.legend(loc=0)\n",
    "        \n",
    "    # plot ground truth curve\n",
    "    plt.plot(xx, (xx * m + b) - tml, color='k', lw=2)\n",
    "    plt.title(title)\n",
    "    \n",
    "    return fig1, plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caso 0**. Las hipótesis se cumplen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0\n",
    "\n",
    "# Agregemos error normal a los datos\n",
    "t0_ = t_ + np.random.randn(n, 1)\n",
    "plot_data_truth(t0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos si podemos obtener los estimadores de los parámetros para estos datos.\n",
    "# Usemos sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr0 = LinearRegression()\n",
    "\n",
    "# Para usar sklearn, el array tiene que se un vector columna\n",
    "print(x_.shape)\n",
    "\n",
    "# Ajustamos\n",
    "lr0.fit(x_, t0_)\n",
    "\n",
    "# Y pedimos los coeficientes\n",
    "print('$\\hat\\omega0$ = ',lr0.intercept_[0])\n",
    "print('$\\hat\\omega1$ = ',lr0.coef_[0, 0])\n",
    "\n",
    "# Verificamos que da lo mismo\n",
    "omega1 = np.sum((x_ - x_.mean()) * (t0_ - t0_.mean())) / np.sum((x_ - x_.mean())**2)\n",
    "omega0 = t0_.mean() - omega1 * x_.mean()\n",
    "print(omega0, omega1)\n",
    "\n",
    "plot_data_truth(t0_)\n",
    "tml = xx * omega1 + omega0\n",
    "plt.plot(xx, tml, '-g', label='ML Fit', lw=3, alpha=0.5)\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Calculemos ahora el error\n",
    "sigma_ = compute_covariance(x_, 1.0)\n",
    "vesti0 = var_estimation(xx, sigma_)\n",
    "plt.plot(xx, tml - 2 * np.sqrt(vesti0), ':b')\n",
    "plt.plot(xx, tml + 2 * np.sqrt(vesti0), ':b')\n",
    "plt.title('Caso 0: hipótesis ok')\n",
    "\n",
    "# Veamos los residuos\n",
    "plt.figure()\n",
    "plt.plot(x_, t0_ - (x_ * omega1 + omega0), 'or')\n",
    "plt.axhline(0.0, color='g', lw=4, alpha=0.5)\n",
    "plt.plot(xx, 2 * np.sqrt(vesti0), ':b')\n",
    "plt.plot(xx, -2 * np.sqrt(vesti0), ':b')\n",
    "plt.plot(xx, (xx * m + b) - tml, color='k', lw=2)\n",
    "plt.title('Caso 0: hipótesis ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que los parámetros están cerca, y que el resultado general es bueno.\n",
    "Veamos que pasa cuando se empiezan a romper las hipótesis.\n",
    "\n",
    "**Caso 1** Heterocedasticidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "np.random.seed(20200831)\n",
    "# Agregemos error normal a los datos, con varianza no fija\n",
    "t1_ = t_ + np.random.randn(n, 1) * (1.0 * x_  + 0.4)\n",
    "plot_data_truth(t1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_results(x_, t1_, 'Caso 1: heterocedasticidad', prediction=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caso 2** Errores no normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "np.random.seed(20200830)\n",
    "\n",
    "# Agregemos error sacado de una Student t con dos grados de libertad\n",
    "t2_ = t_ + np.random.standard_t(3, size=[len(t_), 1]) / 3 # Divido por 3 para tener la misma varianza que antes/\n",
    "plot_data_truth(t2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparemos lo que hacemos con lo que se supone que hacemos\n",
    "xx_ = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx_, st.norm.pdf(xx_), label='Normal')\n",
    "plt.plot(xx_, st.t.pdf(xx_, 3), label='Student t (df = 3)')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_results(x_, t2_, 'Caso 2: non-Gaussian errors', prediction=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caso 3** Errores no independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3\n",
    "np.random.seed(20200831)\n",
    "\n",
    "# Agregemos error sacado de una multinormal con covarianza\n",
    "# Uso un kernel cuadrático\n",
    "dx = x_.flatten()[:, None] - x_.flatten()[None, :]\n",
    "tau = 0.04\n",
    "covmat = 0.3 * np.exp(-0.5* dx**2 / tau)\n",
    "print(covmat.shape)\n",
    "error = np.random.multivariate_normal(mean=x_.flatten()*0.0, cov=covmat, size=(1,)).T\n",
    "\n",
    "t3_ = t_ + error + np.random.randn(n, 1) * 0.84\n",
    "# DE nuevo, las constantes están fijadas para que la varianza sea 1\n",
    "plot_data_truth(t3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_results(x_, t3_, 'Caso 3: non-independent errors', prediction=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusión**\n",
    "\n",
    "Cuando las hipótesis no se cumplen, nos ponemos a jugar una lotería. Es decir, podemos llegar a pegarle a la solución real como no. Desaparecen las garantías y las buenas propiedads del estimador de máxima verosimilitud.\n",
    "\n",
    "¿Cómo se resuelve esto? \n",
    "\n",
    "Hay técnicas, como la *Regresión Lineal Generalizada*, que permiten incorporar errores no normales. Pero esto es tema para la Licenciatura en Ciencia de Datos de UNSAM, no para el curso de Aprendizaje Automático. \n",
    "\n",
    "Acá nos van a preocupar otros temas...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### La era de las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hace ya varios años que se viene diciendo que la [teoría está muerta](https://www.wired.com/2008/06/pb-theory/), que entender sistemas complejos es un ejercicio inútil. Que lo mejor que podemos hacer es predcir el comportamiento de estos sistemas.\n",
    "\n",
    "En ese caso, no importa tanto qué sabemos realmente del sistema, sino si podemos representar su comportamiento de manera. Veamos qué podemos decir de la predicción de los datos en los casos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "see_results(x_, t0_, 'Caso 0: hipótesis ok', prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "see_results(x_, t1_, 'Caso 1: heteroscedasticidad', prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "see_results(x_, t2_, 'Caso 2: non-Gaussian errors', prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "see_results(x_, t3_, 'Caso 3: non-independent errors', prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Entonces, vemos que incluso si las hipótesis son incorrectas, el error en la predicción está dominado por los datos, y no por el modelo. Por lo tanto, podemos concentrarnos en predecir datos, incluso si el modelo no satisface muchas de las hipótesis.\n",
    "\n",
    "**Nota**: yo no creo que el método científico esté obsoleto. Si quieren argumentos, acá hay algunos: (https://arxiv.org/pdf/2007.04095.pdf)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal multivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera idea que se nos puede ocurrir es usar otras columnas del conjunto de datos de casas de California.\n",
    "\n",
    "Recordemos un poco, y reconstruyamos los features que andaban mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['rooms_per_household'] = housing.total_rooms/housing.households\n",
    "housing['bedrooms_per_household'] = housing.total_bedrooms/housing.households\n",
    "housing['bedrooms_per_rooms'] = housing.total_bedrooms/housing.total_rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos las mejores columnas en acción\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "j = i & ~housing.bedrooms_per_rooms.isnull()\n",
    "A = pd.plotting.scatter_matrix(housing.loc[j, ['median_income', 'bedrooms_per_rooms', 'median_house_value']], \n",
    "                               diagonal='hist', hist_kwds={'bins': 50, 'histtype':'step'}, alpha=0.2,\n",
    "                              figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos un predictor con estos\n",
    "x1 = housing.median_income.to_numpy()[j]\n",
    "x2 = housing.bedrooms_per_rooms.to_numpy()[j]\n",
    "t = housing.median_house_value.to_numpy()[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la matriz de diseños\n",
    "phi = np.vstack([x1, x2]).T\n",
    "\n",
    "# Instancio el regresor y ajusto\n",
    "lr = LinearRegression()\n",
    "lr.fit(phi, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, ¿cómo nos fue con respecto al anterior?\n",
    "res1 = t - lr.predict(phi)\n",
    "\n",
    "print('Los residuos tienen una dipersión de $ {:.2f}'.format(np.sqrt(np.sum(res1**2) / len(res1))))\n",
    "print('Los residuos con una sola variable preditiva eran $ {:.2f}'.format(np.sqrt(np.sum(res0**2) / len(res0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno puede preguntarse si valió la pena incluir la segunda variable. Esto también es tema para un curso de data science...\n",
    "\n",
    "¡Qué continue la predicción!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una cosa interesante que podemos hacer es usar no solo las variables lineales, sino también potencias de ellas (ahora veremos en las diapos cómo es que esto sigue funcionando).\n",
    "\n",
    "Para eso, podemos usar unos truquitos de `skelearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "phi2 = pf.fit_transform(phi)\n",
    "\n",
    "# Antes\n",
    "print(phi.shape)\n",
    "# Despues\n",
    "print(phi2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PolynomialFeatures` no solo calcula las potencias de grado dos de las variables, sino también sus cruces. Por eso, al final pasamos de dos variables a seis: $\\{1, x_1, x_2, x_1 x_2, x_1^2, x_2^2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agarramos otro predictor. Miren que ahora no le pido ajustar el intercept.\n",
    "lrpoly = LinearRegression(fit_intercept=False)\n",
    "lrpoly.fit(phi2, t)\n",
    "\n",
    "# Ok, ¿cómo nos fue con respecto al anterior?\n",
    "res2 = t - lrpoly.predict(phi2)\n",
    "\n",
    "print('Los residuos tienen una dipersión de $ {:.2f}'.format(np.sqrt(np.sum(res2**2) / len(res2))))\n",
    "print('Los residuos con una dos variables preditivas eran $ {:.2f}'.format(np.sqrt(np.sum(res1**2) / len(res1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si quieren normalizar\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pp = Pipeline([('scaler', StandardScaler()),\n",
    "               ('poly', PolynomialFeatures(degree=2)),\n",
    "               ('lr', LinearRegression(fit_intercept=False))\n",
    "              ])\n",
    "\n",
    "pp.fit(phi, t)\n",
    "\n",
    "# Ok, ¿cómo nos fue con respecto al anterior?\n",
    "res3 = t - pp.predict(phi)\n",
    "\n",
    "print('Los residuos tienen una dipersión de $ {:.2f}'.format(np.sqrt(np.sum(res3**2) / len(res3))))\n",
    "print('Los residuos con una dos variables preditivas eran $ {:.2f}'.format(np.sqrt(np.sum(res1**2) / len(res1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.hist(res3, 100, density=True, histtype='step', label='2 variables')\n",
    "b = plt.hist(res0, 100, density=True, histtype='step', label='1 variable')\n",
    "\n",
    "# Podemos graficar una Gaussiana encima\n",
    "# Vamos a plotear la _mejor_\n",
    "xx = np.linspace(res2.min(), res2.max(), 100)\n",
    "plt.plot(xx, st.norm.pdf(xx, res2.mean(), res2.std()), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, el escaleo no sirve de mucho.\n",
    "\n",
    "Pero en general la cosa sigue mejorando! Vamos bien. \n",
    "**Volvámonos locos!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = j & ~housing.total_bedrooms.isnull()\n",
    "hh = housing.loc[k].drop(axis=1, columns=['median_house_value', 'ocean_proximity'])\n",
    "t = housing.loc[k, 'median_house_value']\n",
    "\n",
    "# Let's go crazy\n",
    "lr = LinearRegression()\n",
    "lr.fit(hh, t)\n",
    "\n",
    "resN = t - lr.predict(hh)\n",
    "print('Los residuos tienen una dipersión de $ {:.2f}'.format(np.sqrt(np.sum(resN**2) / len(resN))))\n",
    "\n",
    "a = plt.hist(res3, 100, density=True, histtype='step', label='2 variables')\n",
    "b = plt.hist(resN, 100, density=True, histtype='step', label='N variables')\n",
    "\n",
    "# Podemos graficar una Gaussiana encima\n",
    "# Vamos a plotear la _mejor_\n",
    "xx = np.linspace(res2.min(), res2.max(), 100)\n",
    "plt.plot(xx, st.norm.pdf(xx, res2.mean(), res2.std()), '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fit(hh, t)\n",
    "\n",
    "resN = t - pp.predict(hh)\n",
    "print('Los residuos tienen una dipersión de $ {:.2f}'.format(np.sqrt(np.sum(resN**2) / len(resN))))\n",
    "\n",
    "a = plt.hist(res3, 100, density=True, histtype='step', label='2 variables')\n",
    "b = plt.hist(resN, 100, density=True, histtype='step', label='N variables')\n",
    "\n",
    "# Podemos graficar una Gaussiana encima\n",
    "# Vamos a plotear la _mejor_\n",
    "xx = np.linspace(res2.min(), res2.max(), 100)\n",
    "plt.plot(xx, st.norm.pdf(xx, res2.mean(), res2.std()), '-')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vean como de a poco la cosa se va mejorando. Claro, que es porque aumentamos muchísimo el número de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos once variables \"de base\"\n",
    "print('Entrada: {}'.format(hh.shape))\n",
    "\n",
    "# Al crear las features polinomiales\n",
    "print('Fatures: {}'.format(pp.named_steps['poly'].fit_transform(hh).shape))\n",
    "\n",
    "# Veamos cómo aumenta este número\n",
    "degrees = [2, 3, 4, 5, 6]\n",
    "\n",
    "nparams = []\n",
    "for d in degrees:\n",
    "    poly = PolynomialFeatures(degree=d)\n",
    "    nparams.append(poly.fit_transform(hh).shape[1])\n",
    "    \n",
    "plt.plot(degrees, nparams, 'o-r', mfc='None', ms=10, mew=2)\n",
    "plt.xlabel('Grado de features polinomiales')\n",
    "plt.ylabel('Número de features / parámetros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Pregunta**\n",
    "\n",
    "¿Qué creen que pasaría si llego a tener alrededor de 20'000 features?\n",
    "\n",
    "*Aguanten para la respuesta*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explorar la pregunta de arriba con más detalle, vamos a un caso simple, de muchos menos parámetros. Además, esto lo hace más ágil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "# Nuevo set de datos (esta vez, hago dos copias)\n",
    "x_ = np.random.rand(20, 1)\n",
    "t_ = np.sin(2*np.pi*x_) + np.random.randn(len(t_), 1) * 0.3\n",
    "\n",
    "# Antes que nada, voy a separar en conjunto de entrenamiento y de testeo\n",
    "x_train, x_test = x_[:10], x_[10:]\n",
    "t_train, t_test = t_[:10], t_[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array para plotear\n",
    "xx = np.linspace(0, 1, 100).reshape([-1, 1])\n",
    "\n",
    "def plot_data_sine(x, t, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(x, t, 'ob', mfc='None', ms=10)\n",
    "    ax.plot(xx, np.sin(2*np.pi * xx), 'g-', lw=2, alpha=0.7, label='Ground Truth')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('t')\n",
    "    ax.legend(loc=0)\n",
    "    return\n",
    "\n",
    "plot_data_sine(x_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer regresión de estos datos con features polinomiales de orden cada vez mayor.\n",
    "\n",
    "Para eso, creo un pipeline de `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# Creo el pipeline\n",
    "pp_sine = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('lr', LinearRegression(fit_intercept=False))])\n",
    "\n",
    "# Inicializo listas\n",
    "degrees = [1, 2, 3, 5, 7, 9, 11]\n",
    "coeffs = []\n",
    "rmse = []\n",
    "preds = []\n",
    "# Itero sobre los grados\n",
    "for d in degrees:\n",
    "    # Fijo el grado\n",
    "    pp_sine.named_steps['poly'].degree = d\n",
    "    \n",
    "    # Fiteo (y registro los valores de los parámetros)\n",
    "    pp_sine.fit(x_train, t_train)\n",
    "    coeffs.append(pp_sine.named_steps['lr'].coef_)\n",
    "    \n",
    "    # Obtengo predicciones\n",
    "    y_train = pp_sine.predict(x_train)\n",
    "    preds.append(pp_sine.predict(xx))\n",
    "    \n",
    "    # Calculo el RMSE    \n",
    "    rmse.append(np.sqrt(mse(t_train, y_train)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo evoluciona la métrica con el número de grados de libertad\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(degrees, rmse, 'o-r', mfc='None', ms=10, mew=2)\n",
    "plt.xlabel('Grado de features polinomiales')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    print(d, rmse[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-size:16pt\">¡Bien!</p> \n",
    "\n",
    "O sea que cuanto mayor el grado del polinomio, mejor. Hasta llegar a cero.... ¿o no?\n",
    "\n",
    "Veamos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = np.int(np.ceil(len(degrees)/ncols))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    ax = fig.add_subplot(nrows, ncols, i+1)\n",
    "    plot_data_sine(x_train, t_train, ax=ax)\n",
    "    ax.plot(xx, preds[i], 'r-', label='Prediction')\n",
    "    ax.set_ylim(-1.3, 1.3)\n",
    "    ax.set_title('Grado {}'.format(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
